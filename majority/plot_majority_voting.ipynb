{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_percentage(percentage):\n",
    "    \"\"\"Formats percentage with two decimal points\"\"\"\n",
    "    return f\"{percentage * 100:.1f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(ground_truth, final_output):\n",
    "    correct = True\n",
    "    for i in range(len(final_output)):\n",
    "        for j in range(len(final_output[0])):\n",
    "            try: \n",
    "                if final_output[i][j] != ground_truth[i][j]:\n",
    "                    correct = False\n",
    "            except: \n",
    "                correct = False\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LaTeX for rendering text\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "})\n",
    "\n",
    "def plot_success_matrix(success_matrix, task_names):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    # Transpose the success_matrix\n",
    "    success_matrix = success_matrix.T\n",
    "\n",
    "    # Create a color map where unsuccessful runs are shown in light color\n",
    "    cmap = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "    # Display the heatmap with the chosen color map\n",
    "    cax = ax.matshow(success_matrix, cmap=cmap)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xlabel('Task Name')\n",
    "    ax.set_ylabel('Run Index')\n",
    "\n",
    "    # Configure ticks\n",
    "    # Use task names for x-ticks\n",
    "    ax.set_xticks(np.arange(len(task_names)))\n",
    "    ax.set_xticklabels(task_names, rotation=90, fontsize=8)\n",
    "    ax.set_yticks(np.arange(success_matrix.shape[0]))\n",
    "\n",
    "    # Show grid\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.xaxis.set_tick_params(width=0.5)\n",
    "    ax.yaxis.set_tick_params(width=0.5)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plot.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZED 56 TASKS\n",
      "CORRECT 10 TASKS\n",
      "ACCURACY: 0.17857142857142858\n",
      "DIMENSION ACCURACY: 0.875\n",
      "CELL ACCURACY: 0.7101735015772871\n",
      "87.5 & 71.0 & 10 & 17.9 \\\\\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "['e21a174a', '2072aba6', 'ca8de6ea', 'bc4146bd', 'e133d23d', '626c0bcc']\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_performance():\n",
    "    folder = \"results/evaluation_small/no_replace_comma/gpt-3.5-turbo-16k\"\n",
    "    #folder = \"results/evaluation_small/no_replace_comma/gpt-4\"\n",
    "\n",
    "\n",
    "    result_files = os.listdir(folder)\n",
    "    result_files = [os.path.join(folder, file) for file in result_files if os.path.splitext(file)[1] == \".json\"]\n",
    "    accuracies = []\n",
    "    task_names = []\n",
    "    \n",
    "    # We assume there are 5 runs\n",
    "    num_runs = 5\n",
    "    num_tasks = len(result_files)\n",
    "    success_matrix = np.zeros((num_tasks, num_runs))\n",
    "\n",
    "    correct_dimensions, correct_predictions, correct_cells, total_predictions, total_cells, all_correct = 0, 0, 0, 0, 0, []\n",
    "    for i, result_file in enumerate(result_files):\n",
    "\n",
    "        task_name = result_file.split(\"/\")[-1].split(\"_out\")[0]\n",
    "        true_task_path = os.path.join(\"data/evaluation_small/\", task_name + \".json\")\n",
    "        task_names.append(task_name)\n",
    "\n",
    "        ground_truth = None \n",
    "\n",
    "        with open(true_task_path, \"r\") as f:\n",
    "            obj = json.loads(f.read())\n",
    "            ground_truth = obj[\"test\"][0][\"output\"]\n",
    "\n",
    "        with open(result_file, \"r\") as f:\n",
    "            results = json.loads(f.read())[\"output\"]\n",
    "\n",
    "            considered_results = []\n",
    "            dimensions = []\n",
    "            #results = [results[0]]\n",
    "            # for each result filter that all row dimensions are the same\n",
    "            for run_idx in range(num_runs):\n",
    "                result = results[run_idx]\n",
    "                try:\n",
    "                    result =  ast.literal_eval(result)\n",
    "                except:\n",
    "                    continue\n",
    "                d1 = len(result)\n",
    "                if not isinstance(result[0], list): \n",
    "                    continue\n",
    "\n",
    "                d2 = len(result[0])\n",
    "                invalid = False \n",
    "\n",
    "                for row in result: \n",
    "\n",
    "                    if len(row) != d2: \n",
    "                        invalid = True \n",
    "\n",
    "                if not invalid: \n",
    "                    considered_results.append(result)\n",
    "                    dimensions.append((d1, d2))\n",
    "\n",
    "\n",
    "            # perform majority voting on the number of dimensions\n",
    "            majority_dimension = max(dimensions,key=dimensions.count)\n",
    "            # filter to lists with majority dimension\n",
    "            considered_results = [result for result in considered_results if len(result) == majority_dimension[0] and len(result[0]) == majority_dimension[1]]\n",
    "            final_output = considered_results[0]\n",
    "            # majority voting on final output -> we perform character by character voting\n",
    "            for i in range(len(final_output)):\n",
    "                for j in range(len(final_output[0])):\n",
    "\n",
    "                    char_counter = []\n",
    "                    for elm_list in considered_results:\n",
    "                        char_counter.append(elm_list[i][j])\n",
    "\n",
    "                    max_character =  max(char_counter,key=char_counter.count)\n",
    "                    final_output[i][j] = max_character\n",
    "\n",
    "            \n",
    "            if len(final_output) == len(ground_truth) and all(\n",
    "                        len(pred_row) == len(gt_row) for pred_row, gt_row in zip(final_output, ground_truth)):\n",
    "                    correct_dimensions += 1\n",
    "                    for pred_row, gt_row in zip(final_output, ground_truth):\n",
    "                        for pred_cell, gt_cell in zip(pred_row, gt_row):\n",
    "                            total_cells += 1\n",
    "                            if pred_cell == gt_cell:\n",
    "                                correct_cells += 1\n",
    "                                \n",
    "            correct = True\n",
    "            for i in range(len(final_output)):\n",
    "                for j in range(len(final_output[0])):\n",
    "                    try: \n",
    "                        if final_output[i][j] != ground_truth[i][j]:\n",
    "                            correct = False\n",
    "                    except: \n",
    "                        correct = False\n",
    "                        \n",
    "            total_predictions += 1\n",
    "            if correct: \n",
    "                correct_predictions += 1\n",
    "                for idx in range(num_runs):\n",
    "                    current_pred = results[idx]\n",
    "                    if str(current_pred) == str(current_pred):\n",
    "                        success_matrix[i, idx] = 1\n",
    "                accuracies.append(1)\n",
    "            else: \n",
    "                accuracies.append(0)\n",
    "    return correct_dimensions, correct_predictions, correct_cells, total_predictions, total_cells, all_correct, success_matrix, task_names\n",
    "\n",
    "dim_correct, acc_correct, c_acc_correct, total, c_acc_total, all_correct, success_matrix, task_names = evaluate_model_performance()\n",
    "cell_accuracy = c_acc_correct/c_acc_total\n",
    "total_accuracy = acc_correct/total\n",
    "dim_accuracy = dim_correct/total\n",
    "\n",
    "print(f\"ANALYZED {total} TASKS\")\n",
    "print(f\"CORRECT {acc_correct} TASKS\")\n",
    "print(f\"ACCURACY: {total_accuracy}\")\n",
    "print(f\"DIMENSION ACCURACY: {dim_accuracy}\")\n",
    "print(f\"CELL ACCURACY: {cell_accuracy}\")\n",
    "print(f\"{format_percentage(dim_accuracy)} & {format_percentage(cell_accuracy)} & {acc_correct} & {format_percentage(total_accuracy)} \\\\\\\\\")\n",
    "\n",
    "print(success_matrix)\n",
    "\n",
    "# Filter task names that have at least one correct classification\n",
    "filtered_task_names = [name for idx, name in enumerate(task_names) if success_matrix[idx].any()]\n",
    "print(filtered_task_names)\n",
    "# Filter success_matrix for tasks with at least one correct classification\n",
    "filtered_success_matrix = success_matrix[[idx for idx, _ in enumerate(task_names) if success_matrix[idx].any()]]\n",
    "\n",
    "# Plot the matrix\n",
    "plot_success_matrix(filtered_success_matrix, filtered_task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
