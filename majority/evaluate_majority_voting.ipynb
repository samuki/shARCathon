{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_percentage(percentage):\n",
    "    \"\"\"Formats percentage with two decimal points\"\"\"\n",
    "    return f\"{percentage * 100:.1f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZED 57 TASKS\n",
      "CORRECT 11 TASKS\n",
      "ACCURACY: 0.19298245614035087\n",
      "DIMENSION ACCURACY: 0.8596491228070176\n",
      "CELL ACCURACY: 0.7610866959648422\n",
      "86.0 & 76.1 & 11 & 19.3 \\\\\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_performance():\n",
    "    folder1 = \"results/evaluation_small/no_replace_comma/gpt-3.5-turbo-16k\"\n",
    "    folder2 = \"results/evaluation_small/no_replace_comma/gpt-4\"\n",
    "    folder3 = \"results/evaluation_small/no_replace_comma/gpt-3.5-turbo-16k_2\"\n",
    "\n",
    "    folders = [folder1, folder2, folder2]\n",
    "\n",
    "    result_files = os.listdir(folder2)\n",
    "    result_files = [file for file in result_files if os.path.splitext(file)[1] == \".json\"]\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    correct_dimensions, correct_predictions, correct_cells, total_predictions, total_cells, all_correct = 0, 0, 0, 0, 0, []\n",
    "    for i, result_file in enumerate(result_files):\n",
    "\n",
    "        task_name = result_file.split(\"/\")[-1].split(\"_out\")[0]\n",
    "        true_task_path = os.path.join(\"data/evaluation_small/\", task_name + \".json\")\n",
    "        \n",
    "\n",
    "        ground_truth = None \n",
    "\n",
    "        with open(true_task_path, \"r\") as f:\n",
    "            obj = json.loads(f.read())\n",
    "            ground_truth = obj[\"test\"][0][\"output\"]\n",
    "\n",
    "        results = []\n",
    "        for folder in folders:\n",
    "            path = os.path.join(folder, result_file)\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "\n",
    "            with open(path, \"r\") as f:\n",
    "                results += json.loads(f.read())[\"output\"]\n",
    "\n",
    "        considered_results = []\n",
    "        dimensions = []\n",
    "        #results = [results[3]]\n",
    "        # for each result filter that all row dimensions are the same\n",
    "        for result in results:\n",
    "            try:\n",
    "                result =  ast.literal_eval(result)\n",
    "            except:\n",
    "                continue\n",
    "            d1 = len(result)\n",
    "            if not isinstance(result[0], list): \n",
    "                continue\n",
    "\n",
    "            d2 = len(result[0])\n",
    "            invalid = False \n",
    "\n",
    "            for row in result: \n",
    "\n",
    "                if len(row) != d2: \n",
    "                    invalid = True \n",
    "\n",
    "            if not invalid: \n",
    "                considered_results.append(result)\n",
    "                dimensions.append((d1, d2))\n",
    "\n",
    "\n",
    "        # perform majority voting on the number of dimensions\n",
    "        majority_dimension = max(dimensions,key=dimensions.count)\n",
    "        # filter to lists with majority dimension\n",
    "        considered_results = [result for result in considered_results if len(result) == majority_dimension[0] and len(result[0]) == majority_dimension[1]]\n",
    "        final_output = considered_results[0]\n",
    "        # majority voting on final output -> we perform character by character voting\n",
    "        for i in range(len(final_output)):\n",
    "            for j in range(len(final_output[0])):\n",
    "\n",
    "                char_counter = []\n",
    "                for elm_list in considered_results:\n",
    "                    char_counter.append(elm_list[i][j])\n",
    "\n",
    "                max_character =  max(char_counter,key=char_counter.count)\n",
    "                final_output[i][j] = max_character\n",
    "\n",
    "        correct = True\n",
    "        if len(final_output) == len(ground_truth) and all(\n",
    "                    len(pred_row) == len(gt_row) for pred_row, gt_row in zip(final_output, ground_truth)):\n",
    "                correct_dimensions += 1\n",
    "                for pred_row, gt_row in zip(final_output, ground_truth):\n",
    "                    for pred_cell, gt_cell in zip(pred_row, gt_row):\n",
    "                        total_cells += 1\n",
    "                        if pred_cell == gt_cell:\n",
    "                            correct_cells += 1\n",
    "        for i in range(len(final_output)):\n",
    "            for j in range(len(final_output[0])):\n",
    "                try: \n",
    "                    if final_output[i][j] != ground_truth[i][j]:\n",
    "                        correct = False\n",
    "                except: \n",
    "                    correct = False\n",
    "        total_predictions += 1\n",
    "        if correct: \n",
    "            correct_predictions += 1\n",
    "            accuracies.append(1)\n",
    "        else: \n",
    "            accuracies.append(0)\n",
    "    return correct_dimensions, correct_predictions, correct_cells, total_predictions, total_cells, all_correct\n",
    "\n",
    "dim_correct, acc_correct, c_acc_correct, total, c_acc_total, all_correct = evaluate_model_performance()\n",
    "cell_accuracy = c_acc_correct/c_acc_total\n",
    "total_accuracy = acc_correct/total\n",
    "dim_accuracy = dim_correct/total\n",
    "\n",
    "print(f\"ANALYZED {total} TASKS\")\n",
    "print(f\"CORRECT {acc_correct} TASKS\")\n",
    "print(f\"ACCURACY: {total_accuracy}\")\n",
    "print(f\"DIMENSION ACCURACY: {dim_accuracy}\")\n",
    "print(f\"CELL ACCURACY: {cell_accuracy}\")\n",
    "print(f\"{format_percentage(dim_accuracy)} & {format_percentage(cell_accuracy)} & {acc_correct} & {format_percentage(total_accuracy)} \\\\\\\\\")\n",
    "\n",
    "#print(\"accuracy\",  sum(accuracies), \"/\", len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
